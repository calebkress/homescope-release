{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv('austinHousingData.csv')\n",
    "\n",
    "# Convert 'latest_saledate' to datetime, extract numerical features\n",
    "df['latest_saledate'] = pd.to_datetime(df['latest_saledate'])\n",
    "df['sale_year'] = df['latest_saledate'].dt.year\n",
    "df['sale_month'] = df['latest_saledate'].dt.month\n",
    "df['sale_day'] = df['latest_saledate'].dt.day\n",
    "df.drop(columns=['latest_saledate'], inplace=True)\n",
    "\n",
    "# Drop irrelevant columns (0.00 importance)\n",
    "irrelevant_columns = ['streetAddress', 'description', 'latestPriceSource', 'homeImage', 'hasGarage',\n",
    "                      'sale_year', 'sale_month', 'city', 'homeType', 'zpid']\n",
    "df.drop(columns=irrelevant_columns, inplace=True)\n",
    "\n",
    "# Drop columns a user wouldn't likely be able to access\n",
    "user_unavailable_columns = ['numPriceChanges', 'avgSchoolSize', 'avgSchoolRating', 'avgSchoolDistance',\n",
    "                            'numOfPhotos', 'latest_saleyear', 'latest_salemonth', 'sale_day',\n",
    "                            'MedianStudentsPerTeacher', 'numOfElementarySchools', 'numOfHighSchools',\n",
    "                            'numOfMiddleSchools', 'numOfPrimarySchools']\n",
    "df.drop(columns=user_unavailable_columns, inplace=True)\n",
    "\n",
    "# Drop low-importance columns\n",
    "low_importance_columns = ['numOfAccessibilityFeatures', 'numOfCommunityFeatures', 'hasCooling',\n",
    "                          'hasHeating', 'numOfWindowFeatures', 'numOfSecurityFeatures', 'hasView',\n",
    "                          'parkingSpaces', 'propertyTaxRate', 'hasSpa', 'numOfWaterfrontFeatures']\n",
    "df.drop(columns=low_importance_columns, inplace=True)\n",
    "\n",
    "# Add feature interaction columns\n",
    "df['PricePerSqFt'] = df['latestPrice'] / df['livingAreaSqFt']\n",
    "df['BathBedRatio'] = df['numOfBathrooms'] / df['numOfBedrooms']\n",
    "df['LotLivingRatio'] = df['lotSizeSqFt'] / df['livingAreaSqFt']\n",
    "df['GarageBedRatio'] = df['garageSpaces'] / df['numOfBedrooms']\n",
    "df['LatLonInteraction'] = df['latitude'] * df['longitude']\n",
    "\n",
    "# Handle potential division by zero\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "target = 'latestPrice'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Base LightGBM parameters\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# Optimized LightGBM params from Optuna\n",
    "optimized_params = {\n",
    "    **base_params,\n",
    "    'learning_rate': 0.058793153916846676,\n",
    "    'num_leaves': 63,\n",
    "    'max_depth': 16,\n",
    "    'min_data_in_leaf': 11,\n",
    "    'feature_fraction': 0.8957209852243492,\n",
    "    'bagging_fraction': 0.8868935737344777,\n",
    "    'bagging_freq': 7,\n",
    "    'lambda_l1': 0.5825558284248297,\n",
    "    'lambda_l2': 0.005388605992909206\n",
    "}\n",
    "\n",
    "# Adjusted LightGBM params\n",
    "adjusted_params = {\n",
    "    **optimized_params,\n",
    "    'learning_rate': 0.02,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0\n",
    "}\n",
    "\n",
    "# Modular functions for training and evaluation\n",
    "def train_lightgbm(params, train_data, test_data, num_boost_round=1000, early_stopping_rounds=50):\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[train_data, test_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_lightgbm(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Training and evaluation with optimized params\n",
    "print(\"Training with optimized params...\")\n",
    "model = train_lightgbm(optimized_params, train_data, test_data)\n",
    "rmse, mae, r2 = evaluate_lightgbm(model, X_test, y_test)\n",
    "print(f\"Optimized RMSE: {rmse}, MAE: {mae}, R^2: {r2}\")\n",
    "\n",
    "# Training and evaluation with adjusted params\n",
    "print(\"Training with adjusted params...\")\n",
    "model = train_lightgbm(adjusted_params, train_data, test_data, num_boost_round=2000, early_stopping_rounds=100)\n",
    "rmse, mae, r2 = evaluate_lightgbm(model, X_test, y_test)\n",
    "print(f\"Adjusted RMSE: {rmse}, MAE: {mae}, R^2: {r2}\")\n",
    "\n",
    "# Cross-validation to check stability\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = train_lightgbm(adjusted_params, train_data, val_data, num_boost_round=2000, early_stopping_rounds=100)\n",
    "    rmse, mae, r2 = evaluate_lightgbm(model, X_val, y_val)\n",
    "    cv_results.append((rmse, mae, r2))\n",
    "\n",
    "# Aggregate cross-validation results\n",
    "cv_rmse, cv_mae, cv_r2 = map(lambda x: (np.mean(x), np.std(x)), zip(*cv_results))\n",
    "print(f\"Cross-Validation Results - RMSE: {cv_rmse[0]} ± {cv_rmse[1]}, MAE: {cv_mae[0]} ± {cv_mae[1]}, R^2: {cv_r2[0]} ± {cv_r2[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
